{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/28 11:33:33 WARN Utils: Your hostname, DESKTOP-HHC5U09 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/09/28 11:33:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "https://repo1.maven.org/maven2 added as a remote repository with the name: repo-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/root/.pyenv/versions/3.9.13/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dc2f9ed0-a4df-40b9-b73d-edf3fefa2b42;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 141ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dc2f9ed0-a4df-40b9-b73d-edf3fefa2b42\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "25/09/28 11:33:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading Delta Table from MinIO: An error occurred while calling o49.load.\n",
      ": java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.delta.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:191)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource$.parsePathIdentifier(DeltaDataSource.scala:354)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.x$1$lzycompute(DeltaTableV2.scala:75)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.x$1(DeltaTableV2.scala:70)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelByPath$lzycompute(DeltaTableV2.scala:70)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelByPath(DeltaTableV2.scala:70)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$timeTravelSpec$1(DeltaTableV2.scala:108)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelSpec$lzycompute(DeltaTableV2.scala:108)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.timeTravelSpec(DeltaTableV2.scala:104)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$initialSnapshot$1(DeltaTableV2.scala:127)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2$.withEnrichedUnsupportedTableException(DeltaTableV2.scala:367)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot$lzycompute(DeltaTableV2.scala:144)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.initialSnapshot(DeltaTableV2.scala:124)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:236)\n",
      "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:234)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$5(DeltaDataSource.scala:250)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:168)\n",
      "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:166)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:49)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:209)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:346)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 46 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "def get_spark_session():\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"DeltaLakeReader\") \\\n",
    "        .config(\"spark.jars.packages\",\n",
    "                \"io.delta:delta-core_2.12:2.2.0,\"\n",
    "                \"org.apache.hadoop:hadoop-aws:3.3.2,\"\n",
    "                \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "        .config(\"spark.jars.repositories\", \"https://repo1.maven.org/maven2\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9001\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "def read_delta_from_minio(media: str):\n",
    "    \"\"\"\n",
    "    Đọc Delta Table từ MinIO dựa vào media (ví dụ: 'careerviet').\n",
    "\n",
    "    Trả về DataFrame PySpark.\n",
    "    \"\"\"\n",
    "    # Khởi tạo SparkSession\n",
    "    spark = get_spark_session()  # Hàm get_spark_session() của bạn\n",
    "    \n",
    "    # Đường dẫn Delta Table trên MinIO\n",
    "    delta_path = f\"s3a://datn/raw/{media}/company\"\n",
    "\n",
    "    try:\n",
    "        # Đọc Delta Table\n",
    "        df = spark.read.format(\"delta\").load(delta_path)\n",
    "        print(f\"Delta Table loaded from {delta_path}\")\n",
    "        df.show(6)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Delta Table from MinIO: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "read_delta_from_minio('careerviet')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
